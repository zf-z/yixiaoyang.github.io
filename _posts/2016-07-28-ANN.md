---
author: leon
comments: true
date: 2016-07-28 17:34:00+00:00
layout: post
title: '人工神经网络初探'
categories:
- 数据分析
- 机器学习
tags:
- ML
- 聚类
---

### 简介

人工神经网络(Artificial Neural Net，缩写ANN)，现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。不要被唬住了，ANN做的事其实只有`一刀切（单层神经网络）`，一刀切不清楚的就`多切几刀（多层神经网络）`。

#### 计算模型
- 人工神经网络由大量的节点（或称“神经元”，或“单元”）和之间相互联接构成。每个节点代表一种特定的输出函数，称为`激活函数`（activation function）。
- 每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为`权重（weight）`，这相当于人工神经网络的记忆。
- 网络的输出则依网络的连接方式，权重值和激活函数的不同而不同。
- 网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。

人工神经网络通常是通过一个**基于数学统计学类型**的学习方法（Learning Method）得以优化，所以人工神经网络也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题(也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力)，这种方法比起正式的逻辑学推理演算更具有优势。

#### 适用的问题

反向传播是ANN最常用的算法。他适合以下特征的问题：

- 实例以“属性-值”对表示
- 训练数据可能包含错误
- 可以容忍较长的训练时间
- 可能需要快速求出目标值
- 人类能否理解学到的目标函数是不重要的

#### 感知器(神经元)
将感知器算法置于机器学习的更广泛背景中：**感知器属于监督学习算法类别，更具体地说是单层二值分类器**。

神经元（perceptron）示意图：

![neuron-modal.png](http://cdn4.snapgram.co/images/2016/07/28/neuron-modal.png)

- x1~xn为输入向量的各个分量
- w1~wn为神经元各个突触的权值  
- b为偏置
- f(x)为传递函数，通常为非线性函数（加法器）。一般有traingd(),tansig(),hardlim()
- 激活函数，用于限制神经输出振幅，使输出信号压制到一定的区域内。
- y为神经元输出,{-1,+1}

数学表示：<img src="http://latex.codecogs.com/svg.latex?y=f(x)=sign(w*x+b)">

#### 常用激活函数（跃迁函数）

##### 阈值函数(Heaviside函数)
数学表示：<img src="http://latex.codecogs.com/svg.latex?sign(x) =\begin{cases}+1,&x >= 0\\-1,&x < 0\end{cases} ">

##### Logistic函数
数学表示：<img src="http://latex.codecogs.com/svg.latex?f(x)=\frac{1}{1+ae^{-x}}">
Logistic函数输出范围为(0,1)，单调递增，可微分。其中a可以调节倾斜程度。

#### 工作过程

Rosenblatt最初的感知器规则相当简单,可总结为下面两步：

1. 初始化：将权值初始化为0或小随机数。
2. 训练：对每一个训练样本：
    - 计算输出值
    - 更新权值。

权值的更新可以表示为：数学表示：<img src="http://latex.codecogs.com/svg.latex?w_i=w_i+\Delta{w_i}">

在每个增量中，权值的更新值可由如下学习规则得到：<img src="http://latex.codecogs.com/svg.latex?\Delta{w_i}= \eta(t_i-o_i)x_i">
其中ti为目标分类标签，oi为预测标签（感知器输出），η为学习速率（0.0～1.0之间的常数）。简单来说，如果当前输出大于目标输出，则Δw的值为负数，下次计算的值将更偏向目标；反之Δw为正数，下次的输出同样将偏向目标值；η用于控制逼近的速度。

需要注意的是，只有当两个类线性可分才能保证感知器收敛。如果这两个类线性不可分，为避免死循环，我们可以设置一个训练集的最大训练次数，或者是设置一个可接受误分类个数的阈值。


### 使用ANN对鸢尾花数据分类

```python
#!/usr/bin/python
# -*- coding: utf-8 *-*

'''
 Attribute Information:
   0. sepal length in cm
   1. sepal width in cm
   2. petal length in cm
   3. petal width in cm
   4. class:
      -- 0: Iris Setosa
      -- 1: Iris Versicolour
      -- 2: Iris Virginica
'''

import os
import math
import time
import numpy as np
import matplotlib.pyplot as plot
from mpl_toolkits.mplot3d import Axes3D

from scipy import spatial

# 全局变量
g_train_set = None
g_train_target = None
g_test_set = None
g_centroids = None
g_chars = [0,2]
g_data_step = [0, 50, 50, 100]
#g_chars = [1,3]

class Config:
    DATA_FILE = "/devel/git/github/SmallData/LearnMLWithPython/ch02/iris/uic-iris-data/iris.data"
    CLASSES = {
        "Iris-setosa": 0,
        "Iris-versicolor": 1,
        "Iris-virginica": 2
    }
    LABEL_NAMES = ["SepalLen", "SepalWidth", "PetalLen", "PetalWidth", "Classes"]


class Perceptron:
    '''
    基本感知器

    Attributes
    -----------
    '''

    def __init__(self, speed=0.1, train_limit=30, debug=False):
        self.errors = []
        self.speed = speed
        self.debug = debug
        self.train_limit = train_limit
        self.w0 = []

    # 迭代训练函数
    # x:输入向量
    # y:目标向量
    def train(self, x, y, init_param=True):
        if init_param:
            # 每个列数即一个属性，每个属性作为一层，分配一个权值，最后加上一个bias偏置的权值
            # bias可以看作一组（[1,1,..,1],b）的输入，数学上等价
            self.weight = np.zeros(x.shape[1])
            self.bias = 0

        for train_idx in range(self.train_limit):
            errs = 0
            for i, val in enumerate(zip(x, y)):
                xi, ti = val[0], val[1]
                # 计算步进向量
                update = self.speed * (ti - self.active(xi))
                # 更新权重向量和偏置
                self.weight += update * xi
                # bias可以看作一组（[1,1,..,1],b）的输入，数学上等价，所以也需要加上步进
                self.bias += update
                errs += int(update != 0.0)
            self.errors.append(errs)
            self.w0.append(self.weight[0])
            print(self.weight, self.bias, errs)

    # 输入计算函数，返回所有信号输出组成的向量
    def input(self, x):
        # 向量点积计算输入向量xi的输出
        return np.dot(x, self.weight) + self.bias

    # 激活函数，简单的阈值函数即可进行分类
    def active(self, x):
        return np.where(self.input(x) >= 0.0, 1, -1)


def data_import(file, delimiter):
    return np.genfromtxt(file, delimiter=delimiter)

def ann_test():
    pass

# 使用神经网络进行训练，迭代次数限制
def ann_train():
    global g_train_set
    global g_centroids

    x = g_train_set[:, g_chars]
    y = g_train_set[:, [4]]
    ppn = Perceptron(speed=0.1, train_limit=50)
    ppn.train(x, y)

    ann_statics(ppn)

# 统计迭代过程错误次数（理想情况下错误为0即达到收敛目的）
def ann_statics(ppn):
    plot.plot(range(1,len(ppn.errors)+1), ppn.errors, marker='o',color="c")
    plot.plot(range(1,len(ppn.w0)+1), ppn.w0, marker="^", color="k")
    plot.xlabel('Iterations')
    plot.ylabel('W0/Missclassifications')
    plot.show()

def ann_init():
    global g_train_set

    datafile = Config.DATA_FILE
    data = data_import(datafile, ',')

    # 读取并合并训练集数组
    g_train_set = np.hstack((data[g_data_step[0] :g_data_step[1],  0:4], np.full((50, 1), -1.0)))
    g_train_set = np.vstack((g_train_set, np.hstack((data[g_data_step[2]:g_data_step[3], 0:4], np.full((50, 1), 1.0)))))

def ann_plot():
    global g_train_set

    my_set = g_train_set;

    plot.scatter(my_set[0  :50 ,g_chars[0]],my_set[0  :50 ,g_chars[1]],marker="^",color="k")
    plot.scatter(my_set[50 :100,g_chars[0]],my_set[50 :100,g_chars[1]],marker="o",color="m")
    plot.xlabel(Config.LABEL_NAMES[g_chars[0]])
    plot.ylabel(Config.LABEL_NAMES[g_chars[1]])

    plot.autoscale(tight=True)
    plot.grid()
    plot.show()

if __name__ == "__main__":
    # 数据导入
    ann_init()
    ann_plot()
    # 训练数据
    ann_train()
    # 检验测试集
    ann_test()

```

首先使用的数据是0-50,50-100行的数据，先只考虑`sepal length`和`petal length`属性，将原始数据绘制在2D坐标如下：

![iris-ann2.png](http://cdn3.snapgram.co/imgs/2016/07/29/iris-ann2.png)

使用单层网络，2个输入（`sepal length`和`petal length`两个属性组成的输入向量），0-100共100条数据ANN，迭代后发现由于特征突出很快（6次）就达到收敛。

![iris-ann3.png](http://cdn1.snapgram.co/imgs/2016/07/29/iris-ann3.png)

其中黑色为输入向量0的权值，青色为错误归类的样本数。

#### 收敛问题

尽管在较低的学习率情形下，因为一个或多个样本在每一次迭代总是无法被分类造成学习规则不停更新权值，最终，感知器还是无法找到一个好的决策边界。

在这种条件下，感知器算法的另一个缺陷是，一旦所有样本均被正确分类，它就会停止更新权值，这看起来有些矛盾。直觉告诉我们，具有大间隔的决策面（如下图中虚线所示）比感知器的决策面具有更好的分类误差。但是诸如“Support Vector Machines”之类的大间隔分类器不在本次讨论范围。

尽管感知器完美地分辨出两种鸢尾花类，但收敛是感知器的最大问题之一。 Frank Rosenblatt在数学上证明了当两个类可由线性超平面分离时，感知器学习规则收敛，但当类无法由线性分类器完美分离时，问题就出现了。为了说明这个问题，我们将使用鸢尾花数据中另外两个不同的类和特性。选取类为`Iris-versicolor`，`Iris-virginica`，特征为：`sepal width`，`petal width`。

将原始数据绘制在2D坐标如下：

![iris-ann4.png](http://cdn1.snapgram.co/imgs/2016/07/29/iris-ann4.png)

收敛统计：

![iris-ann5.png](http://cdn2.snapgram.co/imgs/2016/07/29/iris-ann5.png)

对于这种不能一刀切的数据，ANN将反复迭代。

### 自适应线性神经元
诚然，感知器被发现时很受欢迎，但是仅仅几年后，Bernard Widrow和他的博士生Tedd Hoff就提出了自适应线性神经元（学习机）的概念。

不同于感知器规则，学习机的delta规则（也被称作学习机的“Widrow-Hoff规则”）基于线性激活函数而不是单位阶跃函数更新权值；在这里，这个线性激活函数g(z)仅作为恒等函数的网络输入。在下一节中，我们会了解为什么线性激活函数能改善感知器更新以及“delta规则”这个命名从何而来。

![](http://cdn3.snapgram.co/imgs/2016/07/29/6941baebjw1er4a2nzmjcj20kb0fytag.jpg)

### Delta定律

### 梯度下降

### 神经网络层次结构

#### 单层前馈网络

#### 多层前馈网络

#### 递归网络

### 其他

### 参考

- [http://python.jobbole.com/81278/](http://python.jobbole.com/81278/)
- [https://zh.wikipedia.org/zh-hans/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C](https://zh.wikipedia.org/zh-hans/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)
